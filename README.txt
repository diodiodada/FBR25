关于梯度更新的方案：

第一种方案：不分享权重，每一个网络单独训练，然后在这25个网络中挑取最优的

第二种方案：权重共享，但是每次训练25个图中只有一个图有输入，其他的图没有输入，更新完一次权重后轮换到下一个图

第三种方案：权重共享，但与2不同，这时25个图都有相同的输入，然后分别求每一个图的梯度，得到25组梯度，然后把他们相加求平均，最后完成一次更新

第四种方案：从这25张图中选取n张图（n未知），然后采取第二种或者第三种方案进行训练

还有一种不太清楚的特殊情况就是：keras 自动对由25张图构成的大图进行了剪枝，减去了重复的边，这种情况可以看成是对25个小图的loss更改了加权


还有，我发现了一些 keras 自带画图工具的不足：
如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入是一样的话，用 plot_model 画图会失败，
而且若这时A和B的输出直接接着网络的输出，feed 数据时 keras 会警告“The list of outputs passed to the model is redundant”

如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入不一样，用 plot_model 画图仍然会失败，
但是若这时A和B的输出直接接着网络的输出，feed 数据时 keras 不会产生这样的警告

还有就是对于我这个任务来说：
若我不使用cell机制，那么模型保存的权重文件大小是：一个cell所需权重大小的15倍
若我使用cell机制，那么模型保存的权重文件大小是：一个cell所需权重大小


我发现了一个现象，就是不分享权重，每一个网络单独训练，然后把每一个网络的输出取平均收敛的很快，但这样做的没什么意义，因为实际应用的时候我们需要的
是单独的把某一个网络的某一个模型拿出来用。
所以测试权重不共享的情况的时候应该把每一个网络的每一个模块单独拿出来测试，然后寻找最优的。