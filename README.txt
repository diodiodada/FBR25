关于梯度更新的方案：

第一种方案：不分享权重，每一个网络单独训练，然后在这25个网络中挑取最优的

第二种方案：权重共享，但是每次训练25个图中只有一个图有输入，其他的图没有输入，更新完一次权重后轮换到下一个图

第三种方案：权重共享，但与2不同，这时25个图都有相同的输入，然后分别求每一个图的梯度，得到25组梯度，然后把他们相加求平均，最后完成一次更新

第四种方案：从这25张图中选取n张图（n未知），然后采取第二种或者第三种方案进行训练

还有一种不太清楚的特殊情况就是：keras 自动对由25张图构成的大图进行了剪枝，减去了重复的边，这种情况可以看成是对25个小图的loss更改了加权


还有，我发现了一些 keras 的不足：

如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入是一样的话，用 plot_model 画图会失败，
但是这里不清楚的是，在计算 loss 的时候 keras 会不会把重复的部分考虑进去。

如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入不一样，用 plot_model 画图仍然会失败，
但是这里可以确定的是，在计算 loss 的时候 keras 会把重复的部分考虑进去。


我发现了一个现象，就是不分享权重，每一个网络单独训练，然后把每一个网络的输出取平均收敛的很快，但这样做的没什么意义，因为实际应用的时候我们需要的
是单独的把某一个网络的某一个模型拿出来用。

所以测试权重不共享的情况的时候应该把每一个网络的每一个模块单独拿出来测试，然后寻找最优的。