关于梯度更新的方案：

第一种方案：不分享权重，每一个网络单独训练，然后在这25个网络中挑取最优的

第二种方案：权重共享，但是每次训练25个图中只有一个图有输入，其他的图没有输入，更新完一次权重后轮换到下一个图

第三种方案：权重共享，但与2不同，这时25个图都有相同的输入，然后分别求每一个图的梯度，得到25组梯度，然后把他们相加求平均，最后完成一次更新

第四种方案：从这25张图中选取n张图（n未知），然后采取第二种或者第三种方案进行训练

还有一种不太清楚的特殊情况就是：keras 自动对由25张图构成的大图进行了剪枝，减去了重复的边，这种情况可以看成是对25个小图的loss更改了加权


还有，我发现了一些 keras 的不足：

如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入是一样的话，用 plot_model 画图不会显示重复的内容，
但是这里不清楚的是，在计算 loss 的时候 keras 会不会把重复的部分考虑进去。

如果 layer(or model) A 和 layer(or model) B 共享权重，并且 A 和 B 的输入不一样，用 plot_model 画图仍然会失败，
但是这里可以确定的是，在计算 loss 的时候 keras 会把重复的部分考虑进去。
